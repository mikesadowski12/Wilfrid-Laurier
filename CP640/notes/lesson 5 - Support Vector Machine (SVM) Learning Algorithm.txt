1 INTRODUCTION
- objective is to build a classifier such that the margin of points in training set from the separating hyper-plane is maximized

2 LINEAR ALGEBRA RECAP
- orthogonal projection to find the distance of a point from a hyper-plane
- This hyper-plane will be the one with a maximum distance from both classes

3 MAXIMIZING THE MARGIN
- For a linearly separable data set, there are infinitely many hyper-planes separating the data
- we would select the one that has maximum distance from both classes
  - Because the hyper-planes that are close to first class (blue circles) can easily mis-classify the green triangles and vice versa
Q: Consider the plot which is generated by the above code. We added a new support vector for the data set. Figure 5 shows the resulting plot. Which ones of the following statements are True and which ones are False? Please explain your answer.
Removing one of the red support vectors (circled red plus points) will not change the decision boundary
Removing one of the non-circled blue points will change the decision boundary
Removing any of the (blue or red) support vectors will change the decision boundary
Removing any of the non-circled (non support vector) points will change the decision boundary
A:
False, removing any of the support vectors will result in a positive slack for one of the constraints of the model. So the decision boundary would completely change.
False, the points that are not a support vector do not have any effect on the decision boundary
True, see explanation for part (a)
False, see explanation for part (b)

4 SOFT-MARGIN SVMS
- Since our data are not linearly separable, we will definitely have a data point that doesnâ€™t satisfy the corresponding constraint for our optimization problem
- To deal with the problem, we will relax our constraints with a penalty in our objective to get a near-optimal infeasible solution
- we will allow support vectors to get closer to the decision boundary and even pass it to the other side
  - these support vectors (circled points) will have a penalty for the objective function of SVM
Q: Consider the soft-margin SVM problem (model 2). Where does a point lie relative to where the margin is depending on the value of ? Specify your answer for each of the following options:
A:
When the slack variable is zero (), then the point lies on the margin or on the correct side of the margin
When , then point lies between the margin. In other words, the point lies on the wrong side of its support vector, but on the correct side of the boundary(margin)
When , the point lies on the wrong side of the decision boundary

5 LAGRANGE DUALITY
- It turns out that the optimization model for SVM is one of those problems that can be solved by Lagrange duality

6 USING LAGRANGIAN ON SVM OBJECTIVE

7 KERNEL TRICK
- functions k(x,y) which implicitly compute dot products between high-dimensional (or even infinite-dimensional) feature vectors; such functions are known as kernels.
- If we can express a learning algorithm purely in terms of dot products, then we can kernelize it by expressing it in terms of kernels; this is known as the kernel trick

8 WHAT CAN BE KERNELIZED
- Theorem (Mercer)

9 APPLICABILITY OF SVM WITH KERNELS
-

Q:  Figure 8 shows 4 SVM classifiers with different regularization parameter and different kernels. Please specify which of the following SVM settings refers to each of the plots.

A soft-margin linear SVM with
A soft-margin linear SVM with
A hard-margin with an RBF kernel with
A hard-margin with an RBF kernel with l = 2

A:
A soft-margin linear SVM with . First, the decision boundry is linear. Second, compared to (c), SVM line separates two classes strictly. The larger regularization term will result such a strict decision boundary.
A hard-margin with an RBF kernel with . A small l corresponds to a small RBF kernel value. This makes the classification hard with few supporting vectors.
A soft-margin linear SVM with . Please refer to explanation of part (a)
A hard-margin with an RBF kernel with l = 1/2. Smaller l will result in a higher number of support vectors.
