1 INTRODUCTION
- these trees are made by putting some tests on nodes of a directed tree (e.g. an attribute being true or false), each edge out of a node represents one of the outcomes, finally leaves represent classes (in a classification problem) or numerical values (in a continuous problem, i.e. regression)
- When using ensembles, we train multiple models and combine their results to get a better performance

2 CLASSIFICATION AND REGRESSION TREE (CART)
- Internal nodes test attributes.
- Branching is determined by attribute value.
- Leaf nodes are outputs (predictions).
Q: Consider the example data set in Figure 2 which is in two dimensions. The data set has two features, x1 and x2. Splitting on which one of the features (x1 or x2) will classify the data correctly? Which split will give the minimum error and how much will be the error?
A: Splitting based on none of the features will result in a 100% accuracy. As a result, none of the features will split the data correctly. However, a splitting based on x2 will result in a minimum error value of 1. Please see the following figure.

3 FIRST ATTEMPT ON THE TREE STRUCTURE
- accuracy is not a good measure for splitting
- There are multiple criteria used for splitting, one important measure is called information gain
- To understand information gain, we first need to know what entropy means

4 ENTROPY
- measures the information (or the “surprise”) of a random variable; denotes how uncertain we are about a random variable that comes from a distribution

4.1 CONDITIONAL ENTROPY AND INFORMATION GAIN
- So conditional entropy shows the expected value of entropy for conditional distribution
- Information gain (mutual information) shows how much uncertainty is reduced (in expectation) about a random variable by knowing another one

4.2 GINI IMPURITY
- We can use other measures for finding the best splitting
- shows how often a randomly chosen element from the set would be incorrectly labeled if it was labeled by the distribution of samples in that node

5 DECISION TREE MISCELLANY

6 ENSEMBLE
- means combining predictions from multiple models to get a better result
  - majority vote or weighted average of predictions
- the used models should differ somehow, either by using completely different models or models that are trained on different training sets or change of hyperparameters
- Bagging means train different models on randomly chosen subsets of data, as we show later this reduces the variance of our model leading to a smaller error
- Boosting refers to training models in sequence, each focusing on the examples that previous models got wrongs

6.1 BAGGING
Q: Suppose, we have a binary classification problem with 3 input features and we apply a bagging algorithm on this data. Suppose each one of our estimators have 60% accuracy. What can we tell about the maximum accuracy that we can gain by using a bagging algorithm?
less than 60%
Equal to 60%
More than 60%
We can’t guess the upper-bound for the final bagging algorithm without seeing the actual data.
A: The correct answer is (c). We can get even 100% accuracy with a bagging algorithm despite having 60% accuracy on each of our weak classifiers. Please consider the following example:

6.2 RANDOM FOREST
- Random Forests (RFs) are an ensemble learning method for both classification and regression tasks
- operate by constructing multiple decision trees (which build-up a forest) at training time
- the mode of the classes resulted from each decision tree will be the output of the random forest algorithm for a classification task
- For regression, the mean of the individual trees will be used as output.
- In other words, random forests are bagged decision trees, with one extra trick to decorrelate the predictions:
  - When choosing each node of the decision tree, choose a random set of input features, and only consider splits on those features.

Q: In random forest algorithm, we generate hundreds of trees and then aggregate the results of these trees. Consider the individual trees in building a random forest classifier. Please specify the statements that are true about these individual trees.

Individual trees select a subset of the features
Individual trees select all the available features
Individual trees select a subset of observations
Individual trees select all of the observations
A: Based on what we learned during the lesson, Items (a) and (c) are True statements.

