1 INTRODUCTION TO NEURAL NETWORKS
- Artificial Neural Networks (ANNs) are some machine learning algorithms that are inspired by the structure and function of a neural network in a human brain
- very powerful learning algorithms and are used in both regression and classification problems and there is a wide range of application of neural network in real-life problems such as speech recognition, computer vision, medical diagnosis, online recommendation systems, video games and many more

1.1 SINGLE NEURON
- A neural network with a single neuron (single node in a single hidden layer) is the simplest version of a Neural Network
- A single neuron with sigmoid activation function can be viewed as a logistic regression classifier where the sigmoid function transforms the input data into a space that can be separated linearly and like logistic regression the goal is to find the parameters wi that minimize the cost function
Q: Consider a single neuron with AND function with two binary features x1 and x2. Table 1 represents the output of AND function for different combinations of input features. Assume the activation function of our single neuron is as follows:
A: C w1=1, w2=1, w3=-2

2 Deep Neural Network
- An input layer takes the input features which can be scalar, a vector, a matrix or even n-dimensional tensor like images
- An arbitrary number of hidden (computation) layers reside between the input and the output layers and the network can have at least zero hidden layers with any number of hidden units (neurons)
- In the case where the layer is the first hidden layer, its input is the input features
- An output layer takes in the last hidden layer and transforms it into the desired output y
- ”layer” refers to a collection of neurons at the specific depth within a neural network and if the number of layers is very large it is called a ”deep neural network”
- Deep learning can be viewed as a way of extracting information in multiple stages to learn data representations
Q: Deep learning can be viewed as a way of extracting information in multiple stages to learn data representations
A: When the number of hidden layers increases

2.1 DEEP FEED-FORWARD NETWORKS
- The primary use of feed-forward networks is for supervised learning algorithms where the data is neither sequential nor time-dependent

2.2 ACTIVATION FUNCTIONS IN NEURAL NETWORKS
- most common activation functions:
  - Linear
  - Sigmoid
  - Tanh or hyperbolic tangent
  - Rectified Linear Unit (ReLU)
  - Leaky ReLU
Q: if we choose linear activation functions in all layers and sigmoid function in output then the model is no more expensive than_________?
A: Logistic Regression

2.3 BACKWARD PROPAGATION (LEARNING IN FEED-FORWARD NETWORKS)
- Learning in feed-forward networks belongs to the scope of supervised learning and generally supervised machine learning models including neural networks consist of inference and training mode operations
- Some possible neural network loss functions are:
  - Mean squared error
  - Cross-entropy cost function

2.4 BATCH GRADIENT DESCENT ALGORITHM
- initialize parameters by randomly sampling from standard normal distribution
- While stopping conditions are False:
  - Randomly select a subsample of the training data named as mini-batch
  - Compute the gradient of loss function with respect to the parameter over all training examples named as backpropagation
  - Update parameters
- Stopping Conditions:
  - Number of iterations: the number of training iterations needed to perform the neural network
- the batch gradient descent suffers from a severe drawback when the data set is very big
- Stochastic mini-batch Gradient Descent:

3.1 DATA SPLIT
- Training split is directly used by the model during model training to minimize the loss function. But as stated earlier, the value of this function is expected to be very low over the set.
- Validation split is used to choose the best hyper-parameters. Just to review, hyper-parameters are parameters which control the model behaviour. Unlike the main algorithm parameters (\(\theta\)), hyper-parameters cannot be directly learned from training data
- Testing split: The neural network never observes this data set during the training or hyper-parameter optimization process. It is only used to evaluate an unbiased estimate of the performance of final architecture and hyper-parameter sets

3.2 BIAS/VARIANCE TRADE OFF
- To Reduce the Effect of Underfitting/ Overfitting in Neural Networks the following strategies can be applied:
  - Underfitting (Training error is high):
    - Train the model longer when the architecture of a network is suitable for the task
    - Use more hidden layers or more parameters per layer when the architecture of the network is too small.
    - Try different architecture when the architecture of the network is not suitable for the task
  - Overfitting (Generalization gap is large)
    - Collect more training data
    - Regularization: modification to the learning algorithm
    - Try different architecture when both above solutions have failed, like reducing the number of hidden layers or neurons at each layer

3.3 NEURAL NETWORK REGULARIZATION
- technique which reduces the overfitting problem and can lead to better performance of learning algorithms
- adds a penalty on different parameters to prevent training a flexible model, therefore the model is less probable to fit the noise and the generalization capability is improved
- Parameter Norm Penalties is the most traditional form of regularization to the neural network
- Dropout is the second powerful regularization technique that is specifically designed for the neural network
- Early Stopping is another form of regularization

3.4 VANISHING GRADIENT
- each of the neural network’s weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training
- in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value
- In the worst case, this may completely stop the neural network from further training



