<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>How to connect Prometheus to OpenShift Streams for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" /><author><name>Pete Muir</name></author><id>8568af74-782f-4e57-8d7b-519de49b8526</id><updated>2021-12-17T07:00:00Z</updated><published>2021-12-17T07:00:00Z</published><summary type="html">&lt;p&gt;You've always been able to view some metrics for &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; in the UI or access them via the API. We recently added a feature to OpenShift Streams for Apache Kafka that exports these metrics to &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt;, a system monitoring and alerting toolkit. Connecting these services lets you view your exported metrics alongside your other Kafka cluster metrics.&lt;/p&gt; &lt;p&gt;This article shows you how to set up OpenShift Streams for Apache Kafka to export metrics to Prometheus. The example configuration includes integration with &lt;a href="https://grafana.com"&gt;Grafana&lt;/a&gt;, a data visualization application that is often used with Prometheus. Almost all observability products and services support Prometheus, so you can easily adapt what you learn in this article to your observability stack.&lt;/p&gt; &lt;h2&gt;Guides and prerequisites&lt;/h2&gt; &lt;p&gt;We will use a number of guides for this configuration; You can open them now or use the links in each section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated: Creating a cluster&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;Configuring a GitHub identity provider&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;OpenShift Dedicated: Managing administration roles and users&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will also use these examples from the Prometheus and Grafana projects, respectively:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;Additional scrape configs for Prometheus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Set up your Kubernetes cluster&lt;/h2&gt; &lt;p&gt;To begin, you need to set up a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster to run Prometheus and Grafana. The example in this article will use &lt;a href="https://cloud.redhat.com/products/dedicated/?intcmp=701f20000012jmYAAQ"&gt;Red Hat OpenShift Dedicated&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Start by following the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated guide&lt;/a&gt; to creating a Customer Cloud Subscription cluster on Amazon Web Services. Then, follow the instruction to &lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;configure a GitHub identity provider&lt;/a&gt; so that you can use your GitHub ID to log in to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. (Other options are available, but we're using these methods for the example.) Once you've got things configured, your OpenShift Cluster Manager should look like the screenshot in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig1.png?itok=B6P_G0hA" width="600" height="147" alt="The OpenShift Cluster Manager with GitHub configured as an identity provider." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Configure GitHub as an identity provider for logging in to the OpenShift Dedicated console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Finally, give your GitHub user the &lt;code&gt;cluster-admin&lt;/code&gt; role by following the OpenShift Dedicated guide to &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;managing administration roles and users&lt;/a&gt;. Note that the Prometheus examples assume you have admin permissions on the cluster. You'll need to use your GitHub username as the principal here.&lt;/p&gt; &lt;p&gt;Now, you can log in to the console using the &lt;strong&gt;Open Console&lt;/strong&gt; button.&lt;/p&gt; &lt;h2&gt;Install Prometheus and Grafana&lt;/h2&gt; &lt;p&gt;You can install Prometheus on OpenShift Dedicated via the OperatorHub. Just navigate to &lt;strong&gt;Operators -&gt; OperatorHub&lt;/strong&gt;, filter for Prometheus, click &lt;strong&gt;Install&lt;/strong&gt;, and accept the defaults. You can validate that it's installed in the &lt;strong&gt;Installed Operators&lt;/strong&gt; list. Once that's done, do the same for Grafana.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You might have command-line muscle memory and prefer to use &lt;code&gt;kubectl&lt;/code&gt; with a Kubernetes cluster. If you want to take this route, switch to a terminal and copy the login command from the OpenShift console user menu to set up your Kubernetes context.&lt;/p&gt; &lt;h2&gt;Set up Prometheus&lt;/h2&gt; &lt;p&gt;To get Prometheus working with OpenShift Streams for Apache Kafka, use the examples in the Prometheus documentation to create an &lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;additional scrape config&lt;/a&gt;. You will need to make a couple of modifications to your configuration.&lt;/p&gt; &lt;h3&gt;Create an additional config for Prometheus&lt;/h3&gt; &lt;p&gt;First, create the additional config file for Prometheus. To do this, create a file called &lt;code&gt;prometheus-additional.yaml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- job_name: "kafka-federate" static_configs: - targets: ["api.openshift.com"] scheme: "https" metrics_path: "/api/kafkas_mgmt/v1/kafkas/&lt;Your Kafka ID&gt;/metrics/federate" oauth2: client_id: "&lt;Your Service Account Client ID&gt;" client_secret: "Your Service Account Client Secret" token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The angle brackets (&lt;code&gt;&lt;&gt;&lt;/code&gt;) in the listing indicate details you'll need to gather from your own OpenShift Streams for Apache Kafka environment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can see your Kafka ID by clicking on your &lt;strong&gt;Kafka Instance&lt;/strong&gt; in the OpenShift Streams for Apache Kafka console.&lt;/li&gt; &lt;li&gt;Follow OpenShift Streams for Apache Kafka &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;getting started guide&lt;/a&gt; to create a Kafka instance and service account for each instance. As described in the guide, copy and paste the client ID and client secret into &lt;code&gt;prometheus-additional.yaml&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Create a Kubernetes secret&lt;/h3&gt; &lt;p&gt;Now, you need to create a Kubernetes secret that contains this config file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f - -n &lt;namespace&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Apply your changes&lt;/h3&gt; &lt;p&gt;Finally, apply &lt;code&gt;prometheus.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role-binding.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role.yaml&lt;/code&gt;, and &lt;code&gt;prometheus-service-account.yaml&lt;/code&gt; using &lt;code&gt;kubectl apply -f &lt;filename&gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Set up Grafana&lt;/h2&gt; &lt;p&gt;To get Grafana working, use the &lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt; example code from the Grafana project's GitHub repository. Remove the &lt;code&gt;hostname&lt;/code&gt; field from the file, as OpenShift Dedicated will assign the hostname automatically.&lt;/p&gt; &lt;p&gt;Now, find the URL for Grafana from the OpenShift console &lt;strong&gt;Routes&lt;/strong&gt; section, and open Grafana. The login details for Grafana are in the Grafana custom resource.&lt;/p&gt; &lt;p&gt;Next, connect Grafana to Prometheus by navigating to &lt;strong&gt;Settings -&gt; Data Sources&lt;/strong&gt;. Click &lt;strong&gt;Add data source&lt;/strong&gt;, then click &lt;strong&gt;Prometheus&lt;/strong&gt;. At that point, all you need to do is enter &lt;code&gt;http://prometheus-operated:9090&lt;/code&gt;, the URL of the service, then click &lt;strong&gt;Save &amp; Test&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You should now find metrics for your Kafka cluster in Grafana. Figure 2 shows a Grafana dashboard that displays most of the metrics available with OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig2.jpg?itok=dF75OCFB" width="600" height="516" alt="A Grafana dashboard for Red Hat OpenShift Streams for Apache Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A sample Grafana dashboard for OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The JSON that defines this dashboard is &lt;a href="https://github.com/pmuir/rhosak-grafana-dashboard"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I've shown you how to use Prometheus and Grafana to observe an OpenShift Streams for Apache Kafka instance. Prometheus is a very widely adopted format for monitoring and can be used with almost all observability services and products.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" title="How to connect Prometheus to OpenShift Streams for Apache Kafka"&gt;How to connect Prometheus to OpenShift Streams for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pete Muir</dc:creator><dc:date>2021-12-17T07:00:00Z</dc:date></entry><entry><title type="html">Using a JAAS realm in Elytron</title><link rel="alternate" href="https://wildfly-security.github.io/wildfly-elytron/blog/jaas-realm/" /><author><name>Diana Krepinska Vilkolakova</name></author><id>https://wildfly-security.github.io/wildfly-elytron/blog/jaas-realm/</id><updated>2021-12-17T00:00:00Z</updated><dc:creator>Diana Krepinska Vilkolakova</dc:creator></entry><entry><title type="html">The Road Towards a Public API (part 2)</title><link rel="alternate" href="https://blog.kie.org/2021/12/the-road-towards-a-public-api-part-2.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2021/12/the-road-towards-a-public-api-part-2.html</id><updated>2021-12-16T10:41:54Z</updated><content type="html">In my I described the principles guiding the design of . As I promised last time, in this blog post I would like to give an overview of new API capabilities that this new design would enable. One downside of having an API that is tightly-coupled with the implementation of the engines is that it is harder to evolve, and it is more difficult to support more than API at the same time. The new API is "", in that each functionality is provided by a component that we called a service. For example a service for evaluating a PMML may be simply: interface PredictionService { DataContext evaluate(Id identifier, DataContext ctx); } Each business asset is pointed to using an identifier. The identifier is constructed as a path, which makes it trivially serializable as a string, easy to share and self-descriptive: one identifier contains most of the relevant meta-data about an asset. For example, to refer to a prediction in PMML: /predictions/my.prediction.id Moreover, the path structure makes it natural to refer to nested items in the same asset. For example, to refer to a task in a specific process, for a given process instance: /processes/my.process.id/instances/my.instance.id/tasks/my.task.id In the following, we will explore future possibilities that the design of the new API enables. Currently NONE of the following features are implemented, nor do we have a timeline for delivery. However, we do plan to explore these capabilities in the future. I hope that, by giving you this sneak peek, you will find these design choices convincing, too! DATACONTEXT One of our first encounters in the API is DataContext. A simple interface that denotes an object that can be transformed into another type. we explored how DataContext may denote both data types such as records, as well of generic key-value pairs (MapDataContext). The DataContext basically comes with only two constraints * the first we already mentioned: DataContext should be convertible into another data context * second, the object it denotes should be serializable (into JSON) These "strong" guarantees are also what enables some of the further extensions that we describe in the following. MULTIPLE SERVICE IMPLEMENTATIONS The service API allows for multiple different interaction models to be provided for the same feature. For instance, a classic model would be a synchronous API: interface PredictionService { DataContext evaluate(Id identifier, DataContext ctx); } But nothing prevents engines to opt-in to provide an asynchronous API; for instance: interface AsyncPredictionService { CompletableFuture&lt;DataContext&gt; evaluate(Id identifier, DataContext ctx); } or even, message-based; imagine: messageBus.send( new ProcessStart( app.get(Processes.class).get($id)), MapDataContext.of(Map.of(...))); messageBus.subscribe( "/processes/$id", // ... subscribe and wait for ProcessStarted event ... ) To the point where a generic interface may be provided too. Each service, may implement an interface that all implementation would respect. For instance: interface AsyncGenericService { CompletableFuture&lt;DataContext&gt; evaluate(Id id, Context ctx); } @Inject AsyncGenericService svc; var futureCtx = svc.evaluate(app.get(Processes.class).get($id), MapDataContext.of(...)); In fact, because the identifier is self-descriptive, and the prefix always denotes, by construction, the "top-level" type of the resource that it points to: /processes/... /predictions/... /decisions/... /rule-units/... …it would be possible to define a "gateway" service, responding to the generic interface above, that would then re-route the actual evaluation to the corresponding service; say: /processes/... -&gt; ProcessService /predictions/... -&gt; PredictionService /decisions/... -&gt; DecisionService /rule-units/... -&gt; RuleUnitService STRUCTURED IDENTIFIERS FOR BEHAVIOR BINDING In fact, structured, self-descriptive identifiers open a whole lot of interesting use cases. If anything, being able to univocally, consistently address any component in the platform, allows for binding behavior to components in a simple way. For instance, we may want to listen for the events that the DMN engine produces; then we may declare a listening class, and bind it to the path: // all processes //(currently equivalent to extending ProcessListener) @Kogito("/processes") class MyEventListener { void onStart() { … } } Now suppose that you want to listen to the behavior of a specific process id @Kogito("/processes/my.process.id") class MyEventListener { void onStart() { … } } But we saw that paths may refer to sub-components, too. For instance, tasks. Suppose that you want to define the implementation of a service task. Then you may bind behavior to it as such: // all processes //(currently equivalent to extending ProcessListener) @Kogito("/processes/my.process.id/task/my.task.id") class MyTaskHandler { @Inject ProcessInstanceId piid; SomeResult doSomething(SomeInput in); } LOCAL VS REMOTE ID: DISTRIBUTED EXECUTION So far we only described "paths", i.e. "local" identifiers. A local identifier always starts with a leading slash ‘/’ and its prefix indicates the type of resources (e.g. process, DMN, PMML, rules, etc.) The reason why it is useful for the ID to be a path, is that a path can be "mounted" on a host:port pair, yielding… a URI! For instance, imagine a fictional kogito:// scheme, then we may denote: kogito://my-app@my.remote.host:54321/processes/my.process.id to denote a Kogito application called my-app, reachable at my.remote.host on port 54321. Now wouldn’t it be fancy if you were able to invoke my.process.id using the same service interface ? @Inject AsyncGenericService svc; var localId = app.get(Processes.class).get("my.process.id"); var remoteId = RemoteId.of("my-app@my.remote.host:54321", localId); var futureCtx = svc.evaluate( remoteId, MapDataContext.of(...)); the local application may decode the host part, and direct the request to it. The target, may resolve the request locally, by decoding the "local" part; or it may even act as gateway that re-routes the request to another different distributed service. The identifier is trivially serializable into a string, and the DataContext is by definition serializable as well! MESSAGES: DESIGNING A SERVICE INTERFACE In order to allow for such future extensions, interfaces should be kept simple and small. I would advocate for single-purpose, single-verb methods. Same verbs may apply to different semantics, depending on the structure of the identifier. For instance: ABORT [kogito://$host:$port]/processesABORT all processesABORT /processes/$idABORT the process with ID $idABORT /processes/$id/instances/$instance_idABORT the instance $instance_id of process $id EVAL /decisions/$id PAYLOAD: {  “json-of-data”: …  }EVAL /decisions/$id PAYLOAD: {  “json-of-data”: …  } The benefit of one such design is that it is easy to translate these verb/id/payload triples into messages to send over the wire. For instance, For instance, a simple translation scheme could be adopted to naturally map each message onto a cloud event { "specversion" : "1.0", "type" : "org.kie.kogito.process.eval", "source" : "/processes/my.process.id", "id" : "A234-1234-1234", // request id "time" : "2018-04-05T17:31:00Z", // timestamp "datacontenttype" : "text/json", "data" : { // data context "var1" : "value1", "var2" : "value2" } } MAPPING ONTO HTTP VERBS But a small API surface is easier to map onto other API vocabularies. For instance, it may result very easy to map such a set of commands onto HTTP verbs. This design has emerged from the implementation of the REST endpoints that are already available in Kogito today: POSTEvaluate/StartDELETEAbortPATCHUpdateetc…  These simple mappings may even allow to provide a "generic" style of API that all Kogito services may accept. This kind of generic REST API, may make it simpler for generic, multi-purpose clients to be developed. In other words, it would enable a style of REST API interaction that is more reminiscent of the KIE Server in v7, but without its pitfalls (for instance, without the specialized marshalling format) # starts process POST http://$host:$port/processes/$id BODY { "var1" : "value1", "var2" : "value2" } CONCLUSIONS This concludes our whirlwind tour in all the possible futures of Kogito APIs. As for plans, and ongoing work, we are currently working on delivering a , and the stateful rule API will follow. Next, we will work on the new listener interfaces and the binding mechanism that we have described here. There is quite a bit of work to do, still, but things are looking bright! If you are looking forward to getting your hands on all of this, stay tuned! The post appeared first on .</content><dc:creator>Edoardo Vacchi</dc:creator></entry><entry><title>An elegant way to performance test microservices on Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/16/elegant-way-performance-test-microservices-kubernetes" /><author><name>Karan Singh</name></author><id>6fd7abb8-dba1-43c9-a901-973085989feb</id><updated>2021-12-16T07:00:00Z</updated><published>2021-12-16T07:00:00Z</published><summary type="html">&lt;p&gt;Application programming interfaces (&lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt;) are the core system of most services. Client, web, and mobile applications are all built from APIs. They sit on the critical path between an end-user and a service, and they're also used for intra-service communication.&lt;/p&gt; &lt;p&gt;Because APIs are so critical, API performance is also essential. It doesn’t matter how well-built your front-end application is if the API data sources it accesses take several seconds to respond. This is especially true in a world of &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, where services depend on each other to provide data. In my opinion, the best feature your API can offer is great performance.&lt;/p&gt; &lt;p&gt;To measure API performance, you need to benchmark your APIs as reliably as possible, which can be challenging. The optimal approach depends on your performance objectives. In this article, I'll guide you through an elegant process for measuring the performance of backend applications running on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. You'll also learn how to use &lt;a href="https://github.com/tsenart/vegeta"&gt;Vegeta&lt;/a&gt;, a versatile HTTP load testing and benchmarking tool written in &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;. We will deploy Vegeta on OpenShift and run performance tests in both standalone and distributed modes.&lt;/p&gt; &lt;h2&gt;Standalone benchmarking with Vegeta&lt;/h2&gt; &lt;p&gt;To run performance tests, you'll need an API endpoint to test. I've provided a simple Go-based application that you will deploy on OpenShift. Once the application is deployed, we'll apply various loads using Vegeta, as Figure 1 illustrates.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/elegant-fig1x2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/elegant-fig1x2.png?itok=HxJMGv16" width="600" height="162" alt="What happens when you trigger a performance test from your local machine." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Triggering a performance test from your local machine. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can follow along with this example on your own OpenShift cluster if you have access to one; otherwise, you can use the &lt;a href="https://developers.redhat.com/developer-sandbox/get-started"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which is free of charge with a Red Hat account.&lt;/p&gt; &lt;h3&gt;Set up the example application&lt;/h3&gt; &lt;p&gt;To begin, log in to your OpenShift cluster from the command line and run the following commands to create a simple GET API in Go.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project perf-testing oc new-app golang~https://github.com/sclorg/golang-ex.git --name=golang-service1 oc expose deployment/golang-service1 --port=8888 oc expose service/golang-service1 oc get route golang-service1 curl http://$(oc get route golang-service1 -o json | jq -r .spec.host)&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Set up the Vegeta benchmarking environment&lt;/h3&gt; &lt;p&gt;Next, install Vegeta on your local machine. Use this command if you're running macOS:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;brew update &amp;&amp; brew install vegeta &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Use this command on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;wget https://github.com/tsenart/vegeta/releases/download/v12.8.4/vegeta_12.8.4_linux_amd64.tar.gz -O /tmp/vegeta.tar.gz tar -xvf /tmp/vegeta.tar.gz sudo mv vegeta /usr/local/bin/ &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Launch your benchmarking process&lt;/h3&gt; &lt;p&gt;Now, you're ready to launch the benchmarking process:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;echo "GET http://$(oc get route golang-service1 -o json | jq -r .spec.host)" | vegeta attack -duration=60s | vegeta report &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;How to read the Vegeta output&lt;/h2&gt; &lt;p&gt;Vegeta's output is largely straightforward; here's an example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;## Output Requests [total, rate, throughput] 3000, 50.02, 49.84 Duration [total, attack, wait] 1m0s, 59.978s, 214.968ms Latencies [min, mean, 50, 90, 95, 99, max] 204.638ms, 217.337ms, 214.49ms, 222.256ms, 227.075ms, 394.248ms, 492.278ms Bytes In [total, mean] 51000, 17.00 Bytes Out [total, mean] 0, 0.00 Success [ratio] 100.00% Status Codes [code:count] 200:3000 Error Set:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Some of the more important metrics here are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Requests&lt;/strong&gt;: The total number of requests, their rate per second, and their throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latencies&lt;/strong&gt;: The time taken to send the requests and the time taken to wait for the response.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Success&lt;/strong&gt;: The percentage of requests that were successful.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Status Code&lt;/strong&gt;: The status code and the number of requests that were successful.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To summarize the output of this first test: When we attempted to access the single pod of the service &lt;code&gt;golang-service1&lt;/code&gt; over the internet, we found a mean latency of around 217 milliseconds at 50 requests per second. This is a good indication that the application is working as expected.&lt;/p&gt; &lt;h2&gt;Testing API performance with Vegeta&lt;/h2&gt; &lt;p&gt;Now, let's get more serious. Run a test with 64 parallel workers, without any throttling or rate-limiting:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;echo "GET http://$(oc get route golang-service1 -o json | jq -r .spec.host)"| vegeta attack -duration=60s -rate=0 -max-workers=64 | vegeta report &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Here's the output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;## Output Requests [total, rate, throughput] 17908, 298.44, 297.38 Duration [total, attack, wait] 1m0s, 1m0s, 214.692ms Latencies [min, mean, 50, 90, 95, 99, max] 201.543ms, 214.793ms, 214.57ms, 222.524ms, 224.861ms, 228.75ms, 563.581ms Bytes In [total, mean] 304436, 17.00 Bytes Out [total, mean] 0, 0.00 Success [ratio] 100.00% Status Codes [code:count] 200:17908 Error Set:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the load increased to 64 threads, we got a mean latency of around 214 milliseconds at 298 requests per second—a rate per second that's six times higher than what we saw in the previous test. The latency basically stayed constant (it actually dipped just a bit) as the number of requests per second increased, which is great.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: We are stress-testing a Golang app running on a single pod, hosted on a shared OpenShift cluster over the internet (in this case, the cluster is hosted on the Developer Sandbox). This is just an example to show you how to quickly run a performance test against your own application; it does not represent the real-world performance of any component.&lt;/p&gt; &lt;h2&gt;Benchmarking Kubernetes service names in a cluster&lt;/h2&gt; &lt;p&gt;In the previous test, you benchmarked an internet-facing service endpoint. In this test, you'll use the locally accessible Kubernetes service name and run a performance test against that, as illustrated in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/graphic2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/graphic2.png?itok=v-L-0gG8" width="600" height="196" alt="What happens when you trigger a microservices performance test from within an OpenShift cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Triggering a microservices performance test from within an OpenShift cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Launch Vegeta as a pod in the same namespace (project) as your service, then run the same test that you ran previously:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc run vegeta --rm --attach --restart=Never --image="quay.io/karansingh/vegeta-ubi" -- sh -c \ "echo 'GET http://golang-service1:8888' | vegeta attack -duration=60s -rate=0 -max-workers=64 | vegeta report" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's the output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;## Output If you don't see a command prompt, try pressing enter. Requests [total, rate, throughput] 732977, 12205.54, 12205.23 Duration [total, attack, wait] 1m0s, 1m0s, 1.514ms Latencies [min, mean, 50, 90, 95, 99, max] 201.313µs, 3.133ms, 472.767µs, 1.751ms, 3.585ms, 80.58ms, 102.89ms Bytes In [total, mean] 12460609, 17.00 Bytes Out [total, mean] 0, 0.00 Success [ratio] 100.00% Status Codes [code:count] 200:732977 Error Set:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The service is now seeing about 12,000 requests per second, and the mean latency is 3 milliseconds. These improved results should come as no surprise: All the traffic is staying within OpenShift, unlike the previous test in which Vegeta connected to the Golang service over the internet.&lt;/p&gt; &lt;h2&gt;A distributed load test for parallel containerized workloads&lt;/h2&gt; &lt;p&gt;Next, let's try a benchmarking test that's closer to a real-world example. You'll run the same test again, using the Golang application's Kubernetes service name. But this time, you'll launch multiple Vegeta pods, all hammering your backend microservice in parallel, as illustrated in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/graphic3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/graphic3.png?itok=k6NFnbt7" width="600" height="239" alt="What happens when you trigger a distributed performance test on your microservices." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Triggering a distributed performance test on your microservices. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Start by scaling the Go application deployment to 10 replicas, which should make things more interesting:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc scale deployment/golang-service1 --replicas=10&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The best way to launch a distributed load test is to use OpenShift's &lt;code&gt;Job&lt;/code&gt; object, which provides the flexibility to launch parallel containerized workloads. Create a YAML file named &lt;code&gt;vegeta-job.yaml&lt;/code&gt; with the following content. This sets &lt;code&gt;parallelism&lt;/code&gt; to 10 pods, which will launch 10 Vegeta pods, which will in turn launch attacks on the Golang service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;apiVersion: batch/v1 kind: Job metadata: name: vegeta spec: parallelism: 10 completions: 10 backoffLimit: 0 template: metadata: name: vegeta spec: containers: - name: vegeta image: quay.io/karansingh/vegeta-ubi command: ["/bin/sh","-c"] args: ["echo 'GET http://golang-service1:8888' | vegeta attack -duration=60s -rate=0 -max-workers=64 | tee /tmp/results.bin ; sleep 600" ] restartPolicy: OnFailure &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply this file to the OpenShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create -f vegeta-job.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait a minute or two for Vegeta to complete its test run. Then, execute the following command, which will import and aggregate the binary output files from all 10 Vegeta pods onto your local machine (where you installed the Vegeta binary at the beginning of this article) and generate a final performance report:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for i in $(oc get po | grep -i vegeta | awk '{print $1}') ; do oc cp $i:tmp/results.bin $i.bin &amp; done ; fg vegeta report *.bin ; &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Here's the output from the distributed test:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;## Output Requests [total, rate, throughput] 5651742, 88071.28, 88070.66 Duration [total, attack, wait] 1m4s, 1m4s, 449.276µs Latencies [min, mean, 50, 90, 95, 99, max] 69.075µs, 5.538ms, 1.476ms, 16.563ms, 27.235ms, 47.554ms, 333.121ms Bytes In [total, mean] 96079614, 17.00 Bytes Out [total, mean] 0, 0.00 Success [ratio] 100.00% Status Codes [code:count] 200:5651742 Error Set:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this test, the Golang service delivered a mean latency of around 5.5 milliseconds at 88,070 requests per second, which works out to about 5.2 million requests per minute. That's pretty impressive performance.&lt;/p&gt; &lt;h2&gt;Tidy up your environment&lt;/h2&gt; &lt;p&gt;After my experiments, I like to clean up my system. You can tidy up your own machine with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc delete -f vegeta-job.yaml oc delete project perf-testing &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;There is a great saying from the physicist and engineer Lord Kelvin: "If you cannot measure it, you cannot improve it." In this article, you've learned an elegant method for testing API performance in your distributed microservices applications. You can use the techniques introduced here to benchmark your next great backend microservice application running on OpenShift or Kubernetes.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/16/elegant-way-performance-test-microservices-kubernetes" title="An elegant way to performance test microservices on Kubernetes"&gt;An elegant way to performance test microservices on Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Karan Singh</dc:creator><dc:date>2021-12-16T07:00:00Z</dc:date></entry><entry><title>JBoss Tools and Red Hat CodeReady Studio for Eclipse 2021-09 security fix release for Apache Log4j CVE-2021-44228</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/12.21.1.ga.html" /><category term="release" /><category term="jbosstools" /><category term="devstudio" /><category term="jbosscentral" /><category term="codereadystudio" /><author><name>jeffmaury</name></author><id>https://tools.jboss.org/blog/12.21.1.ga.html</id><updated>2021-12-18T09:51:47Z</updated><published>2021-12-16T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div class="sect1"&gt; &lt;h2 id="apache-log4j-cve-2021-44228-statement"&gt;&lt;a class="anchor" href="#apache-log4j-cve-2021-44228-statement"&gt;&lt;/a&gt;Apache Log4j CVE-2021-44228 statement&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A remote code execution vulnerability in the Apache Log4j 2 Java library dubbed Log4Shell (see &lt;a href="https://logging.apache.org/log4j/2.x/index.html#News" class="bare"&gt;https://logging.apache.org/log4j/2.x/index.html#News&lt;/a&gt; or &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-44228" class="bare"&gt;https://nvd.nist.gov/vuln/detail/CVE-2021-44228&lt;/a&gt;) has been releaved to the public audience on Friday Dec 9th, 2021.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We’ve released new versions of JBoss Tools and Red Hat CodeReady Studio that switches the version of Apache Log4j to 2.16.0.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We highly recommand to download or update to this latest version.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/blog/images/crstudio12.png" alt="crstudio12" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="installation"&gt;&lt;a class="anchor" href="#installation"&gt;&lt;/a&gt;Installation&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Red Hat CodeReady Studio comes with everything pre-bundled in its installer. Simply download it from our &lt;a href="https://developers.redhat.com/products/codeready-studio/overview/"&gt;Red Hat CodeReady product page&lt;/a&gt; and run it like this:&lt;/p&gt; &lt;/div&gt; &lt;div class="literalblock"&gt; &lt;div class="content"&gt; &lt;pre&gt;java -jar codereadystudio-&amp;lt;installername&amp;gt;.jar&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;JBoss Tools or Bring-Your-Own-Eclipse (BYOE) CodeReady Studio require a bit more:&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This release requires at least Eclipse 4.21 (2021-09) but we recommend using the latest &lt;a href="https://www.eclipse.org/downloads/packages/release/2021-09/r/eclipse-ide-enterprise-java-and-web-developers"&gt;Eclipse 4.21 2021-09 JEE Bundle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;since then you get most of the dependencies preinstalled.&lt;/p&gt; &lt;/div&gt; &lt;div class="admonitionblock warning"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java11 is now required to run Red Hat Developer Studio or JBoss Tools (this is a requirement from Eclipse 4.17). So make sure to select a Java11 JDK in the installer. You can still work with pre-Java11 JDK/JRE and projects in the tool.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once you have installed Eclipse, you can either find us on the Eclipse Marketplace under &amp;quot;JBoss Tools&amp;quot; or &amp;quot;Red Hat CodeReady Studio&amp;quot;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;For JBoss Tools, you can also use our update site directly.&lt;/p&gt; &lt;/div&gt; &lt;div class="literalblock"&gt; &lt;div class="content"&gt; &lt;pre&gt;http://download.jboss.org/jbosstools/photon/stable/updates/&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Apache Log4j CVE-2021-44228 statement A remote code execution vulnerability in the Apache Log4j 2 Java library dubbed Log4Shell (see https://logging.apache.org/log4j/2.x/index.html#News or https://nvd.nist.gov/vuln/detail/CVE-2021-44228) has been releaved to the public audience on Friday Dec 9th, 2021. We’ve released new versions of JBoss Tools and Red Hat CodeReady Studio that switches the version of Apache Log4j to 2.16.0. We highly recommand to download or update to this latest version. Installation Red Hat CodeReady Studio comes with everything pre-bundled in its installer. Simply download it from our Red Hat CodeReady product page and run it like this: java -jar codereadystudio-&lt;installername&gt;.jar JBoss Tools or Bring-Your-Own-Eclipse (BYOE) CodeReady Studio require a bit more: This...</summary><dc:creator>jeffmaury</dc:creator><dc:date>2021-12-16T00:00:00Z</dc:date></entry><entry><title>How to use MirrorMaker 2 with OpenShift Streams for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/15/how-use-mirrormaker-2-openshift-streams-apache-kafka" /><author><name>Pete Muir</name></author><id>72915b43-4904-4698-adff-3976a20687c5</id><updated>2021-12-15T07:00:00Z</updated><published>2021-12-15T07:00:00Z</published><summary type="html">&lt;p&gt;Recently, I had to help a customer get Apache Kafka MirrorMaker 2, which replicates data from one &lt;a href="https://developers.redhat.com/topics/kafka/all"&gt;Apache Kafka&lt;/a&gt; cluster to another, working with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. I used &lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-strimzi-apache-kafka-on-kubernetes-kubecon-europe-2020"&gt;Strimzi&lt;/a&gt;, a project focused on running Apache Kafka on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. While the process was relatively straightforward, the configuration was a bit tricky.&lt;/p&gt; &lt;p&gt;In this article, I'll share the steps I took to get MirrorMaker 2 and OpenShift Streams for Apache Kafka working together. The specific configuration I used will serve as our example.&lt;/p&gt; &lt;h2&gt;What is Apache Kafka MirrorMaker 2?&lt;/h2&gt; &lt;p&gt;Apache Kafka MirrorMaker 2 is a cross-cluster data replication (mirroring) technology. It's built on top of &lt;a href="https://developers.redhat.com/topics/event-driven/connectors"&gt;Apache Kafka Connect&lt;/a&gt;, which provides increased scalability and reliability over Apache Kafka MirrorMaker 1. MirrorMaker 2 can be used for migration, backup, disaster recovery, and failover&lt;/p&gt; &lt;h2&gt;Guides and prerequisites&lt;/h2&gt; &lt;p&gt;We will use a number of guides for this configuration; You can open them now or use the links in each section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated: Creating a cluster&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;Configuring a GitHub identity provider &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;OpenShift Dedicated: Managing administration roles and users&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/ee92cfdb-9587-42f8-80d5-54169e0e3c07"&gt;Configuring and connecting Kafkacat with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that you will need a no-cost &lt;a href="https://www.redhat.com/en/program-developers"&gt;Red Hat Developer account&lt;/a&gt; to access some of the resources provided in this article.&lt;/p&gt; &lt;h2&gt;Set up your Apache Kafka clusters&lt;/h2&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Our example is based on a common use case for MirrorMaker: Migrating from a self-managed Kafka cluster to a cloud service. In this case, your source would be your self-managed Kafka instance while your target would be OpenShift Streams for Apache Kafka. However, to simplify the example, I created both the source and target using OpenShift Streams for Apache Kafka.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Begin by creating both source and target Kafka clusters using OpenShift Streams for Apache Kafka. You can use two different Red Hat accounts for this in order to take advantage of the &lt;a href="https://www.redhat.com/en/products/trials"&gt;trial instances&lt;/a&gt; offered.&lt;/p&gt; &lt;p&gt;For each Red Hat account, follow the guide to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;, creating a Kafka instance and a service account. As described in the guide, copy the bootstrap server, client ID, and client secret for each, and save copies to a temporary location to use later.&lt;/p&gt; &lt;p&gt;MirrorMaker needs broad access to Kafka, so we'll need fairly liberal permissions for this example, as shown in Figure 1. In production, you could probably get away with making the permissions more restrictive—for instance, you could set them so that all topics could be read, and then only add write permissions to the topics that MirrorMaker uses to manage the replication.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mirrormaker-fig1_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mirrormaker-fig1_0.png?itok=Dqftgkn4" width="600" height="90" alt="The permissions required are Allow All on Consumer Group is * for the service account and Allow All on Topic is * for the service account." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The permissions required for the source Kafka cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;For the target cluster, give the service account you created similar permissions, as shown in Figure 2. These permissions include the ability to alter the Kafka instance so that the access control lists can be replicated.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mirrormaker-fig2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mirrormaker-fig2.png?itok=z8o5CHSR" width="600" height="114" alt="The permissions required for the target Kafka cluster are Allow All on Consumer Group is * for the service account, Allow All on Topic is * for the service account and Allow Alter on the Kafka instance for the Service Account." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The permissions required for the target Kafka cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Finally, create a topic, &lt;code&gt;foo&lt;/code&gt;, and accept the defaults, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mirrormaker-fig3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mirrormaker-fig3.png?itok=Fn_MbC9C" width="600" height="665" alt="The defaults for the topic foo created on the source Kafka cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The default configuration values for the topic foo. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Set up OpenShift Dedicated&lt;/h2&gt; &lt;p&gt;You'll need a Kubernetes cluster in order to run Strimzi. This example uses &lt;a href="https://cloud.redhat.com/products/dedicated"&gt;Red Hat OpenShift Dedicated&lt;/a&gt;, but you could also use &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/container-platform"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;, or you could install Strimzi yourself on any Kubernetes distribution.&lt;/p&gt; &lt;p&gt;Following the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;Creating a Cluster&lt;/a&gt; guide, create a Customer Cloud Subscription cluster on Amazon Web Services (AWS). Then follow the &lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;Configuring a GitHub identity provider&lt;/a&gt; guide so you can use your GitHub ID to log in to OpenShift. (Other options are available, but our example is based on this login method.) The results should look like Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mirrormaker-fig4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mirrormaker-fig4.png?itok=ljqPv--5" width="600" height="147" alt="The cluster manager shows that GitHub is configured as an identity provider." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Validate that you have configured GitHub as an identity provider in Red Hat OpenShift Cluster Manager. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Finally, give this GitHub user the &lt;code&gt;dedicated-admins&lt;/code&gt; role by following the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;Managing administration roles and users&lt;/a&gt; guide. The result should be what you see in Figure 5. Note that you need to use your GitHub username as the principal here.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mirrormaker-fig5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mirrormaker-fig5.png?itok=DJP3WaIs" width="600" height="224" alt="The cluster managers shows that the GitHub username is the dedicated-admin role." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Validate that you have given your GitHub username the dedicated-admins role. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Now you can log into the console using the &lt;strong&gt;Open Console&lt;/strong&gt; button.&lt;/p&gt; &lt;h2&gt;Set up Strimzi&lt;/h2&gt; &lt;p&gt;To install Strimzi on OpenShift Dedicated, navigate to &lt;strong&gt;Operators -&gt; OperatorHub&lt;/strong&gt;. Filter for Strimzi, click &lt;strong&gt;Install&lt;/strong&gt;, and accept the defaults. You can validate that Strimzi is installed by checking the &lt;strong&gt;Installed Operators&lt;/strong&gt; list. If you prefer to use the &lt;code&gt;kubectl&lt;/code&gt; command-line interface with a Kubernetes cluster, you can switch to a terminal and copy the login command from the OpenShift console user menu to set up your Kubernetes context.&lt;/p&gt; &lt;p&gt;Now we have setup Strimzi, and two OpenShift Streams for Apache Kafka instance our major elements as shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Blank%20diagram.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Blank%20diagram.png?itok=qyQHOLMI" width="600" height="543" alt="An overview diagram, showing the two Red Hat OpenShift Streams for Apache Kafka intances and Apache Kafka MirrorMaker 2 running on OpenShift Dedicated" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. Overview of the system architecture &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Configure MirrorMaker 2&lt;/h3&gt; &lt;p&gt;Strimzi uses Kubernetes custom resources to configure MirrorMaker. Load the following code into your editor; note that in this listing and elsewhere, code that you'll need to modify with information specific to your setup is surrounded by angle brackets (&lt;code&gt;&lt; &gt;&lt;/code&gt;). Save the file to disk as &lt;code&gt;mm2.yml&lt;/code&gt;. The snippet includes inline comments to help you understand the configuration. In particular, note that you're reducing the sync interval to 60 seconds. This will make debugging for development easier because it provides a faster feedback loop. You would probably want to set this back to the default for production use.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaMirrorMaker2 metadata: name: my-mirror-maker2 spec: clusters: - alias: my-cluster-source authentication: clientId: "&lt;Source Client ID&gt;" # The Client ID for the service account for the source Kafka cluster clientSecret: # A reference to a Kubernetes Secret that contains the Client Secret for the service account for the source Kafka cluster key: client-secret secretName: source-client-secret tokenEndpointUri: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token" type: oauth # Red Hat OpenShift Streams for Apache Kafka prefers OAuth for connections bootstrapServers: "&lt;Source Cluster Bootstrap server&gt;" # The bootstrap server host for the source cluster tls: # Red Hat OpenShift Streams for Apache Kafka requires the use of TLS with the built in trusted certificates trustedCertificates: [] - alias: my-cluster-target authentication: clientId: "&lt;Target Client ID&gt;" # The Client ID for the service account for the target Kafka cluster clientSecret: # A reference to a Kubernetes Secret that contains the Client Secret for the service account for the target Kafka cluster key: client-secret secretName: target-client-secret tokenEndpointUri: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token" type: oauth # Red Hat OpenShift Streams for Apache Kafka prefers OAuth for connections bootstrapServers: "&lt;Target Cluster Bootstrap server&gt;" # The bootstrap server host for the source cluster config: # Red Hat OpenShift Streams for Apache Kafka requires a replication factor of 3 for all topics config.storage.replication.factor: 3 offset.storage.replication.factor: 3 status.storage.replication.factor: 3 tls: # Red Hat OpenShift Streams for Apache Kafka requires the use of TLS with the built in trusted certificates trustedCertificates: [] connectCluster: my-cluster-target mirrors: - checkpointConnector: config: checkpoints.topic.replication.factor: 3 # Red Hat OpenShift Streams for Apache Kafka requires a replication factor of 3 for all topics emit.checkpoints.interval.seconds: 60 # Setting sync interval to 60 seconds is useful for debugging refresh.groups.interval.seconds: 60 # Setting sync interval to 60 seconds is useful for debugging sync.group.offsets.enabled: true # Enable sync'ing offsets sync.group.offsets.interval.seconds: 60 # Setting sync interval to 60 seconds is useful for debugging sourceCluster: my-cluster-source sourceConnector: config: refresh.topics.interval.seconds: 60 # Red Hat OpenShift Streams for Apache Kafka requires a replication factor of 3 for all topics replication.factor: 3 # Red Hat OpenShift Streams for Apache Kafka requires a replication factor of 3 for all topics sync.topic.acls.enabled: true # Enable sync'ing offsets targetCluster: my-cluster-target topicsPattern: .* # Sync all topics replicas: 1 # Running a single replica of MirrorMaker makes debugging the logs easier &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create your secrets&lt;/h3&gt; &lt;p&gt;As you can see in the configuration file, you will need a couple of secrets for storing the client authentication for each cluster. You'll create those next. First, create the secret for the source cluster as follows, using your client secret for the service account you created for the source cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl create secret generic source-client-secret --from-literal=client-secret=&lt;Source Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create the secret for the target cluster, using your client secret for the service account you created for the target cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl create secret generic target-client-secret --from-literal=client-secret=&lt;Target Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, apply the MirrorMaker configuration you created above:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f mm2.yml &lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Check your work&lt;/h3&gt; &lt;p&gt;To ensure that everything worked, run &lt;code&gt;kubectl get kmm2 -o yaml&lt;/code&gt; and check that the &lt;code&gt;condition&lt;/code&gt; has &lt;code&gt;status: True&lt;/code&gt;. If you encounter problems, you may have copied some of the configurations incorrectly, or the authorization could be wrong. The MirrorMaker logs don't highlight errors very well (they tend to be in there as warnings) but if you dig through them, you should be able to find a pointer.&lt;/p&gt; &lt;p&gt;If you check the target cluster in the OpenShift Streams for Apache Kafka UI, you should now see a number of topics created by MirrorMaker, including one called &lt;code&gt;my-cluster-source.foo&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Test MirrorMaker's synchronization&lt;/h2&gt; &lt;p&gt;To confirm that messages are being synchronized, follow the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/ee92cfdb-9587-42f8-80d5-54169e0e3c07"&gt;Kafkacat&lt;/a&gt; guide to producing and consuming messages. In this case, you'll produce the message on the source cluster and consume it from the target cluster.&lt;/p&gt; &lt;p&gt;Open one terminal window for each cluster. In the terminal for the source cluster, set these three environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;export BOOTSTRAP_SERVER=&lt;Source cluster Bootstrap server&gt; export CLIENT_ID=&lt;Source Client ID&gt; export CLIENT_SECRET=&lt;Source Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the terminal for the target cluster, set these three environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;export BOOTSTRAP_SERVER=&lt;Target cluster Bootstrap server&gt; export CLIENT_ID=&lt;Target Client ID&gt; export CLIENT_SECRET=&lt;Target Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now go back to the first terminal and produce a message to the source cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kcat -t foo -b $BOOTSTRAP_SERVER -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -P&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the second terminal, consume it from the target cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kcat -t my-cluster-source.foo -b "$BOOTSTRAP_SERVER" -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" -&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There may be a bit of delay caused by MirrorMaker 2's synchronization period.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I've shown you how to use Apache Kafka MirrorMaker 2 to replicate data between two OpenShift Streams for Apache Kafka instances, using Strimzi to configure and run MirrorMaker 2 on an OpenShift Dedicated cluster.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/15/how-use-mirrormaker-2-openshift-streams-apache-kafka" title="How to use MirrorMaker 2 with OpenShift Streams for Apache Kafka"&gt;How to use MirrorMaker 2 with OpenShift Streams for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pete Muir</dc:creator><dc:date>2021-12-15T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus is not affected by the Log4J Vulnerability</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-and-CVE-2021-4428/" /><author><name>Max Rydahl Andersen</name></author><id>https://quarkus.io/blog/quarkus-and-CVE-2021-4428/</id><updated>2021-12-15T00:00:00Z</updated><content type="html">As many of you have heard, the Java community has been rocked by a widespread vulnerability in the Apache Log4J 2 logging library. You can find more details about this vulnerability in CVE-2021-44228 Since Quarkus, its extensions, and dependencies do not use the log4j version 2 core library, it is...</content><dc:creator>Max Rydahl Andersen</dc:creator></entry><entry><title>Install OpenShift's Web Terminal Operator in any namespace</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/14/install-openshifts-web-terminal-operator-any-namespace" /><author><name>Josh Pinkney</name></author><id>bdfbcf50-b931-4049-b61a-22151f7aa4e9</id><updated>2021-12-14T07:00:00Z</updated><published>2021-12-14T07:00:00Z</published><summary type="html">&lt;p&gt;The Web Terminal Operator in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; provides a web terminal with common cluster tooling pre-installed. The operator gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally.&lt;/p&gt; &lt;p&gt;This article is an overview of the new features introduced in Web Terminal Operator 1.4. One of the most important improvements is that you can now install the Web Terminal Operator in any namespace. In addition, our tooling has been updated to be compatible with OpenShift 4.9.&lt;/p&gt; &lt;h2&gt;Install the Web Terminal Operator in any namespace&lt;/h2&gt; &lt;p&gt;Previously, the Web Terminal Operator had to be installed in the &lt;code&gt;openshift-operators&lt;/code&gt; namespace in order to successfully interface with the OpenShift console. As of OpenShift 4.9, we've made changes to both the OpenShift console and the Web Terminal Operator to allow the Web Terminal Operator to work when installed not just in &lt;code&gt;openshift-operators&lt;/code&gt;, but in any other namespace as well.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you decide to install the operator in a namespace other than &lt;code&gt;openshift-operators&lt;/code&gt;, be aware that this comes with some security risks. Anyone who has access to the namespace where the Web Terminal Operator is installed can change default tooling images.&lt;/p&gt; &lt;p&gt;If you are looking to enable this feature, you must create an operator group and a subscription through YAML, as illustrated in the listing below, rather than through the OpenShift UI.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Create the new namespace oc new-project new-web-terminal-operator-namespace # Create the operator group cat &lt;&lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: web-terminal-non-openshift-operators EOF # Create the subscription cat &lt;&lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: web-terminal spec: channel: fast installPlanApproval: Automatic name: web-terminal source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: web-terminal.v1.4.0 EOF &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Tooling update&lt;/h2&gt; &lt;p&gt;We have updated the default binaries in Web Terminal Operator 1.4 to include the latest versions of the built-in command-line tools, as shown in Table 1.&lt;/p&gt; &lt;p class="Indent1"&gt;Table 1. Command-line tools in Web Terminal Operator 1.4.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="388"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Binary&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Old version&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;New version&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;oc&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4.8.2&lt;/td&gt; &lt;td&gt;4.9.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;odo&lt;/code&gt;&lt;/td&gt; &lt;td&gt;2.2.3&lt;/td&gt; &lt;td&gt;2.3.1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;knative&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.21.0&lt;/td&gt; &lt;td&gt;0.23.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;tekton&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.17.2&lt;/td&gt; &lt;td&gt;0.19.1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;rhoas&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.25.0&lt;/td&gt; &lt;td&gt;0.34.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;submariner&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.9.1&lt;/td&gt; &lt;td&gt;0.10.1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;For a peek into how the Web Terminal Operator works under the hood, please see &lt;a href="http://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;A deeper look at the Web Terminal Operator&lt;/a&gt; by Angel Misevski. Joshua Wood's initial release article, &lt;a href="https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview"&gt;Command-line cluster management with Red Hat OpenShift’s new web terminal&lt;/a&gt;, is worth a read as well. You can also refer to our previous release blog, &lt;a href="https://developers.redhat.com/articles/2021/08/19/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13"&gt;What's new in Red Hat OpenShift's Web Terminal Operator 1.3&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/14/install-openshifts-web-terminal-operator-any-namespace" title="Install OpenShift's Web Terminal Operator in any namespace"&gt;Install OpenShift's Web Terminal Operator in any namespace&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Josh Pinkney</dc:creator><dc:date>2021-12-14T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.2.2 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-2-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-2-2</id><updated>2021-12-14T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.2.2 has just been released.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">KIE &amp;amp; Log4j2 exploit CVE-2021-44228</title><link rel="alternate" href="https://blog.kie.org/2021/12/kie-log4j2-exploit-cve-2021-44228.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2021/12/kie-log4j2-exploit-cve-2021-44228.html</id><updated>2021-12-13T18:25:30Z</updated><content type="html">2.x is a widely used Java logging framework. Unfortunately a few days ago it has been exposed to an important (“Log4Shell”, CVE-2021-44228).  The whole KIE ecosystem (Kogito, Drools, OptaPlanner and jBPM) moved to , a different logging facade with Logback as default implementation, a few years ago and it is therefore not vulnerable by CVE-2021-44228. Accordingly, our recommendation is to ensure your applications are updated to the latest community versions (at the time of writing, Drools, jBPM, KIE Workbench/Business Central and KIE Server 7.62.0.Final, Kogito 1.14.1.Final, Optaplanner 8.14.0.Final). Therefore if you’re using KIE projects as libraries in your projects you are not affected by this problem. Note that the , only declares the Log4j2 dependency management without actually depending on it. Dashbuilder is a monitoring component included in Business Central. We are about to just in case. In case you’re declaring and/or using Log4j2 dependency in your own KIE projects, please make sure to upgrade Log4j2 as soon as possible to version 2.15.0 which solves this problem.  We invite you to monitor this blog post, which will be updated in case of any future additional findings. Further readings: Update note: We found that Dashbuilder brought in log4j-core as a transitive dependency from the module uberfire-metadata-backend-elasticsearch and it has been . The latest version of KIE Workbench/Business Central containing it was 7.46.0.Final. Update note 2: As reported , the fact that WildFly distribution, and then also KIE Workbench/Business Central one which is based on it, contains the log4j-api artifact does not have any security implication. In fact the vulnerable code is only present in the log4j-core module which is not part of the distribution. The post appeared first on .</content><dc:creator>Mario Fusco</dc:creator></entry></feed>
